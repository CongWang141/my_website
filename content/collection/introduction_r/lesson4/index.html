---
title: "Lesson 4"
author: "Cong Wang & Renan Serenini"
date: "2022-08-25"
draft: false
excerpt: 
weight: 1
links:
- icon: github
  icon_pack: fab
  name: code
  url: https://github.com/CongWang141/statistics-with-r.git
---



<div id="author" class="section level3">
<h3>author</h3>
<p>Cong Wang &amp; Renan Serenini</p>
</div>
<div id="solution-to-lesson-3" class="section level2">
<h2>Solution to lesson 3</h2>
<ol style="list-style-type: decimal">
<li>Consider the model <span class="math inline">\(y=\beta_0+\beta_1x+\mu\)</span></li>
<li>Define a value to the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, creating these objects</li>
</ol>
<pre class="r"><code>b0=3
b1=4</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><p>Generate the variable x, drawing a sample (size=50) from a normal distribution</p></li>
<li><p>Generate the variable <span class="math inline">\(\mu\)</span>, drawing from the standard normal distribution</p></li>
<li><p>Create the variable <span class="math inline">\(y=\beta_0+\beta_1x+\mu\)</span></p></li>
</ol>
<pre class="r"><code>x=rnorm(n=50, mean = 5, sd=1)
u=rnorm(50)
Y=b0+b1*x+u</code></pre>
<ol start="6" style="list-style-type: decimal">
<li><p>Estimate the parameters via OLS.</p></li>
<li><p>Get the coefficients. The values might be close to the parameters.</p></li>
</ol>
<pre class="r"><code>model1 &lt;- lm(Y~x)

model1$coefficients</code></pre>
<pre><code>## (Intercept)           x 
##    3.433386    3.912986</code></pre>
<pre class="r"><code>model1$coefficients[1] #b0</code></pre>
<pre><code>## (Intercept) 
##    3.433386</code></pre>
<pre class="r"><code>model1$coefficients[2] #b1</code></pre>
<pre><code>##        x 
## 3.912986</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Repeat the previous routine (steps 3 to 7) 1000 times. Tip: use “for”. For every estimation, store the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in empty vectors previously created.</li>
</ol>
<pre class="r"><code>vector_b0 &lt;- numeric(1000)
vector_b1 &lt;- numeric(1000)
for (i in 1:1000) {
  x=rnorm(50, mean = 5, sd=1) 
  u=rnorm(50) 
  Y=b0+b1*x+u 
  model1 &lt;- lm(Y~x)
  vector_b0[i] &lt;- model1$coefficients[1] #get the estimate of b0 and store
  vector_b1[i] &lt;- model1$coefficients[2] #get the estimate of b1 and store
}</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>Get the mean of each vector (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>).</li>
</ol>
<pre class="r"><code>mean(vector_b0)</code></pre>
<pre><code>## [1] 2.998263</code></pre>
<pre class="r"><code>mean(vector_b1)</code></pre>
<pre><code>## [1] 4.000387</code></pre>
<p>Are they close to the true parameters? Are the OLS estimators biased?</p>
<p>#10-Plot the histogram of each of them</p>
<pre class="r"><code>par(mfrow=c(1,2))
hist(vector_b0, main = expression(beta[0]),col=&#39;steelblue&#39;)
hist(vector_b1, main = expression(beta[1]), col = &#39;red&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<ol start="11" style="list-style-type: decimal">
<li>REPEAT ALL THE SIMULATIONS, BUT WITH SAMPLE SIZE EQUAL TO 200, THEN EQUAL TO 1000. Plot the 3 histograms (sample size =50, 200 and 1000) of <span class="math inline">\(\beta_0\)</span> side by side to compare.</li>
</ol>
<pre class="r"><code># SAMPLE SIZE=200
vector_b0_200 &lt;- numeric(1000)
vector_b1_200 &lt;- numeric(1000)
for (i in 1:1000) {
  x=rnorm(200, mean = 5, sd=1) 
  u=rnorm(200) 
  Y=b0+b1*x+u 
  model1 &lt;- lm(Y~x)
  vector_b0_200[i] &lt;- model1$coefficients[1] #get the estimate of b0 and store
  vector_b1_200[i] &lt;- model1$coefficients[2] #get the estimate of b1 and store
}</code></pre>
<pre class="r"><code>#SAMPLE SIZE =1000
vector_b0_1000 &lt;- numeric(1000)
vector_b1_1000 &lt;- numeric(1000)
for (i in 1:1000) {
  x=rnorm(1000, mean = 5, sd=1) 
  u=rnorm(1000) 
  Y=b0+b1*x+u 
  model1 &lt;- lm(Y~x)
  vector_b0_1000[i] &lt;- model1$coefficients[1] #get the estimate of b0 and store
  vector_b1_1000[i] &lt;- model1$coefficients[2] #get the estimate of b1 and store
}</code></pre>
<p>Plot the 3 histograms (sample size =50, 200 and 1000) of b0 side by side to compare.</p>
<p>Do the same with the 3 histograms of <span class="math inline">\(\beta_1\)</span>?</p>
<p>Are the OLS estimators consistent?</p>
<pre class="r"><code>#TIP: USE THE SAME RANGE ON X-AXIS (argument xlim= ) TO A BETTER COMPARISON
par(mfrow=c(1,3))
hist(vector_b0,xlim = c(1,6), main = &quot;Sample=50&quot;)
hist(vector_b0_200, xlim = c(1,6),main = &quot;Sample=200&quot;)
hist(vector_b0_1000,xlim = c(1,6), main = &quot;Sample=1000&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>#b1
hist(vector_b1, xlim = c(3.4,4.6),main = &quot;Sample=50&quot;)
hist(vector_b1_200, xlim = c(3.4,4.6),main = &quot;Sample=200&quot;)
hist(vector_b1_1000, xlim = c(3.4,4.6),main = &quot;Sample=1000&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
</div>
<div id="hypothesis-test-in-ols" class="section level2">
<h2>Hypothesis test in OLS</h2>
<p>Test two-sided hypothesis</p>
<pre class="r"><code># call packages
library(AER)
library(scales)

# load the &quot;CASchools&quot; data
data(CASchools)</code></pre>
<pre class="r"><code># add student-teacher ratio
CASchools$STR &lt;- CASchools$students/CASchools$teachers

# add average test-score
CASchools$score &lt;- (CASchools$read + CASchools$math)/2</code></pre>
<p>The formula for t-test of estimates is
<span class="math display">\[t=\frac{(estimated \: value - hypothesized \: value)}{standard \: error \: of \: the \: estimator}\]</span></p>
<pre class="r"><code># estimate the model and get coefficients
linear_model &lt;- lm(score ~ STR, data = CASchools) 
linear_model$coefficients</code></pre>
<pre><code>## (Intercept)         STR 
##  698.932949   -2.279808</code></pre>
<pre class="r"><code>B1=linear_model$coefficients[2]</code></pre>
<p>standard error of <span class="math inline">\(\hat{beta_1}\)</span></p>
<pre class="r"><code>summary(linear_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***
## STR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06</code></pre>
<pre class="r"><code>ls(summary(linear_model)) #ls gives us the possible outcomes</code></pre>
<pre><code>##  [1] &quot;adj.r.squared&quot; &quot;aliased&quot;       &quot;call&quot;          &quot;coefficients&quot; 
##  [5] &quot;cov.unscaled&quot;  &quot;df&quot;            &quot;fstatistic&quot;    &quot;r.squared&quot;    
##  [9] &quot;residuals&quot;     &quot;sigma&quot;         &quot;terms&quot;</code></pre>
<pre class="r"><code>summary(linear_model)$coefficients  </code></pre>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## STR          -2.279808  0.4798255 -4.751327  2.783308e-06</code></pre>
<pre class="r"><code>summary(linear_model)$coef #just &quot;coef&quot; can be used as well</code></pre>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## STR          -2.279808  0.4798255 -4.751327  2.783308e-06</code></pre>
<pre class="r"><code>linear_model$coefficients</code></pre>
<pre><code>## (Intercept)         STR 
##  698.932949   -2.279808</code></pre>
<pre class="r"><code>coef1=summary(linear_model)$coefficients</code></pre>
<pre class="r"><code>coef1[2] #B1</code></pre>
<pre><code>## [1] -2.279808</code></pre>
<pre class="r"><code>coef1[,2] #Access second column, standard errors for intercept and B1</code></pre>
<pre><code>## (Intercept)         STR 
##   9.4674911   0.4798255</code></pre>
<pre class="r"><code>SEB1= coef1[,2][2] #Second element of second column
#or
SEB11=coef1[4] #4th element in general
SEB1==SEB11 # they are exactly the same</code></pre>
<pre><code>##  STR 
## TRUE</code></pre>
<p>t-statistics by hands</p>
<pre class="r"><code>(t_by_hand=(B1-0)/SEB1) # t-statistics by hands</code></pre>
<pre><code>##       STR 
## -4.751327</code></pre>
<pre class="r"><code>coef1[,3][2] # access from regression results</code></pre>
<pre><code>##       STR 
## -4.751327</code></pre>
<pre class="r"><code>(t_by_hand=(B1-0)/SEB1)==coef1[,3][2] # check if they are the same value</code></pre>
<pre><code>##  STR 
## TRUE</code></pre>
<p>p-value by hands, there are 420 obeservations in our dataset, we have 1 parameter in our model, so the Degrees of freedom in our model is</p>
<pre class="r"><code>#Degrees of freedom = n-k-1
df=420-1-1</code></pre>
<p>Pt function provides the student distribution</p>
<pre class="r"><code>p_value=2*pt(t_by_hand, df=df)
p_value</code></pre>
<pre><code>##          STR 
## 2.783308e-06</code></pre>
<p>NOTE: As we are dealing with a large sample, the normal density could be used as well:</p>
<pre class="r"><code>2*pnorm(t_by_hand)</code></pre>
<pre><code>##          STR 
## 2.020859e-06</code></pre>
<p>We reject the null and conclude that the coefficient is significantly different from zero.</p>
<p>Null: the class size has no influence on the students test scores at 5% level.</p>
</div>
<div id="plotting" class="section level2">
<h2>Plotting</h2>
<p>Plot the standard normal on the support [-6,6]</p>
<pre class="r"><code>t &lt;- seq(-6, 6, 0.01)

plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     lwd = 2, 
     yaxs = &quot;i&quot;, 
     axes = F, 
     ylab = &quot;&quot;, 
     main = expression(&quot;Calculating the p-value of a Two-sided Test when&quot; ~ t^act ~ &quot;=-4.75&quot;), 
     cex.lab = 0.7,
     cex.main = 1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Add x axis</p>
<pre class="r"><code>plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     lwd = 2, 
     yaxs = &quot;i&quot;, 
     axes = F, 
     ylab = &quot;&quot;, 
     main = expression(&quot;Calculating the p-value of a Two-sided Test when&quot; ~ t^act ~ &quot;=-4.75&quot;), 
     cex.lab = 0.7,
     cex.main = 1)

tact &lt;- -4.75
axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Shade the critical regions using <code>polygon()</code></p>
<pre class="r"><code>plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     lwd = 2, 
     yaxs = &quot;i&quot;, 
     axes = F, 
     ylab = &quot;&quot;, 
     main = expression(&quot;Calculating the p-value of a Two-sided Test when&quot; ~ t^act ~ &quot;=-4.75&quot;), 
     cex.lab = 0.7,
     cex.main = 1)

tact &lt;- -4.75
axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)

# critical region in left tail
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = &#39;orange&#39;)

# critical region in right tail

polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = &#39;orange&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Add arrows and texts indicating critical regions and the p-value</p>
<pre class="r"><code>plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     lwd = 2, 
     yaxs = &quot;i&quot;, 
     axes = F, 
     ylab = &quot;&quot;, 
     main = expression(&quot;Calculating the p-value of a Two-sided Test when&quot; ~ t^act ~ &quot;=-4.75&quot;), 
     cex.lab = 0.7,
     cex.main = 1)

tact &lt;- -4.75
axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)

# critical region in left tail
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = &#39;orange&#39;)

# critical region in right tail

polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = &#39;orange&#39;)

arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1)
arrows(3.5, 0.2, 2.5, 0.02, length = 0.1)

arrows(-5, 0.16, -4.75, 0, length = 0.1)
arrows(5, 0.16, 4.75, 0, length = 0.1)

text(-3.5, 0.22, 
     labels = expression(&quot;0.025&quot;~&quot;=&quot;~over(alpha, 2)),
     cex = 0.7)
text(3.5, 0.22, 
     labels = expression(&quot;0.025&quot;~&quot;=&quot;~over(alpha, 2)),
     cex = 0.7)

text(-5, 0.18, 
     labels = expression(paste(&quot;-|&quot;,t[act],&quot;|&quot;)), 
     cex = 0.7)
text(5, 0.18, 
     labels = expression(paste(&quot;|&quot;,t[act],&quot;|&quot;)), 
     cex = 0.7)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Add ticks indicating critical values at the 0.05-level, t^act and -t^act</p>
<pre class="r"><code>plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = &quot;l&quot;, 
     col = &quot;steelblue&quot;, 
     lwd = 2, 
     yaxs = &quot;i&quot;, 
     axes = F, 
     ylab = &quot;&quot;, 
     main = expression(&quot;Calculating the p-value of a Two-sided Test when&quot; ~ t^act ~ &quot;=-4.75&quot;), 
     cex.lab = 0.7,
     cex.main = 1)

tact &lt;- -4.75
axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)

# critical region in left tail
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = &#39;orange&#39;)

# critical region in right tail

polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = &#39;orange&#39;)

arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1)
arrows(3.5, 0.2, 2.5, 0.02, length = 0.1)

arrows(-5, 0.16, -4.75, 0, length = 0.1)
arrows(5, 0.16, 4.75, 0, length = 0.1)

text(-3.5, 0.22, 
     labels = expression(&quot;0.025&quot;~&quot;=&quot;~over(alpha, 2)),
     cex = 0.7)
text(3.5, 0.22, 
     labels = expression(&quot;0.025&quot;~&quot;=&quot;~over(alpha, 2)),
     cex = 0.7)

text(-5, 0.18, 
     labels = expression(paste(&quot;-|&quot;,t[act],&quot;|&quot;)), 
     cex = 0.7)
text(5, 0.18, 
     labels = expression(paste(&quot;|&quot;,t[act],&quot;|&quot;)), 
     cex = 0.7)

rug(c(-1.96, 1.96), ticksize  = 0.145, lwd = 2, col = &quot;darkred&quot;)
rug(c(-tact, tact), ticksize  = -0.0451, lwd = 2, col = &quot;darkgreen&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="confidence-interval" class="section level2">
<h2>Confidence interval</h2>
<p>Generate a sample with 100 obeservations from a normal distribution with mean=5 and standard deviation=5, and plot</p>
<pre class="r"><code>set.seed(1)

Y &lt;- rnorm(n = 100, 
           mean = 5, 
           sd = 5)

plot(Y, 
     pch = 19, 
     col = &quot;steelblue&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Assuming <span class="math inline">\(Y_i= \mu + \epsilon_i\)</span>, the confidence interval is</p>
<pre class="r"><code>cbind(CIlower = mean(Y) - 1.96 * 5 / 10, CIupper = mean(Y) + 1.96 * 5 / 10) #cbind combine objects</code></pre>
<pre><code>##       CIlower  CIupper
## [1,] 4.564437 6.524437</code></pre>
<p>From the same intuition, we can have confidence interval in our previous linear model</p>
<pre class="r"><code>linear_model$coefficients</code></pre>
<pre><code>## (Intercept)         STR 
##  698.932949   -2.279808</code></pre>
<pre class="r"><code>confint(linear_model)</code></pre>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## STR          -3.22298  -1.336636</code></pre>
<p>Let’s do it “by hand”, <code>qt</code> gives the quantile function for the t distribution</p>
<pre class="r"><code>lm_summ &lt;- summary(linear_model)
c(&quot;lower&quot; = lm_summ$coef[2,1] - qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2],
  &quot;upper&quot; = lm_summ$coef[2,1] + qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2])</code></pre>
<pre><code>##     lower     upper 
## -3.222980 -1.336636</code></pre>
</div>
<div id="dummy-regression" class="section level2">
<h2>Dummy regression</h2>
<p>Create a dummy variable and plot it with score</p>
<pre class="r"><code># Create the dummy variable 
CASchools$D &lt;- CASchools$STR &lt; 20
# Plot the data
plot(CASchools$D, CASchools$score,            
     pch = 20,                                
     cex = 0.6,   #set size of plot symbols to 0.6
     col = &quot;Steelblue&quot;,                       
     xlab = expression(D[i]),                 
     ylab = &quot;Test Score&quot;,
     main = &quot;Dummy Regression&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Fit a model and summary</p>
<pre class="r"><code>#Model
dummy_model &lt;- lm(score ~ D, data = CASchools)
summary(dummy_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ D, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  650.077      1.393 466.666  &lt; 2e-16 ***
## DTRUE          7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202</code></pre>
<pre class="r"><code>plot(CASchools$D, CASchools$score,            
     pch = 20,                                
     cex = 0.6,   #set size of plot symbols to 0.6
     col = &quot;Steelblue&quot;,                       
     xlab = expression(D[i]),                 
     ylab = &quot;Test Score&quot;,
     main = &quot;Dummy Regression&quot;)

points(x = CASchools$D,  #Plot the two predicted points
       y = predict(dummy_model), 
       col = &quot;red&quot;, 
       pch = 20,
       cex=1.2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Use a function in R to get the confidence interval</p>
<pre class="r"><code>confint(dummy_model) #Confidence intervals</code></pre>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## DTRUE         3.539562  10.79931</code></pre>
</div>
<div id="heteroskadasticity" class="section level2">
<h2>Heteroskadasticity</h2>
<p>All inference made so far relies on the assumption that error variance does not vary as regressors values changes, which is called homoskadasticity</p>
<pre class="r"><code># load scales package for adjusting color opacities
library(scales)</code></pre>
<p>Generate some Heteroskadasticity data</p>
<pre class="r"><code># set seed for reproducibility
set.seed(1)

# set up vector of x coordinates
x &lt;- rep(c(10, 15, 20, 25), each = 25)

#Vector of errors
e &lt;- c() # Empty Vector. We could also use numeric(100), as before

# sample 100 errors such that the variance increases with x
e[1:25] &lt;- rnorm(25, sd = 10) # by default mean=0
e[26:50] &lt;- rnorm(25, sd = 20)
e[51:75] &lt;- rnorm(25, sd = 30)
e[76:100] &lt;- rnorm(25, sd = 40)

# set up y
y &lt;- 720 - 3.3 * x + e

# Estimate the model 
mod &lt;- lm(y ~ x)

# Plot the data
plot(x = x, 
     y = y, 
     xlab = &quot;Student-Teacher Ratio&quot;,
     ylab = &quot;Test Score&quot;,
     cex = 0.5, 
     pch = 19, 
     xlim = c(8, 27), 
     ylim = c(600, 710))

# Add the regression line to the plot
abline(mod, col = &quot;darkred&quot;)

# Add boxplots to the plot
boxplot(formula = y ~ x, 
        add = TRUE, 
        at = c(10, 15, 20, 25), 
        col = alpha(&quot;gray&quot;, 0.4), 
        border = &quot;black&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>As you can see from the plot, the scores become more and more disperse as the student to teacher ratio goes up, the variance becomes bigger. The data is heteroskadastic.</p>
<p>Real life example for comparison</p>
<pre class="r"><code># load package and attach data
library(AER)
data(&quot;CPSSWEducation&quot;)
attach(CPSSWEducation)

#Overview
summary(CPSSWEducation)</code></pre>
<pre><code>##       age          gender        earnings        education    
##  Min.   :29.0   female:1202   Min.   : 2.137   Min.   : 6.00  
##  1st Qu.:29.0   male  :1748   1st Qu.:10.577   1st Qu.:12.00  
##  Median :29.0                 Median :14.615   Median :13.00  
##  Mean   :29.5                 Mean   :16.743   Mean   :13.55  
##  3rd Qu.:30.0                 3rd Qu.:20.192   3rd Qu.:16.00  
##  Max.   :30.0                 Max.   :97.500   Max.   :18.00</code></pre>
<p>Estimate a simple regression model, plot observations and add the regression line</p>
<pre class="r"><code>labor_model &lt;- lm(earnings ~ education)
plot(education, 
     earnings, 
     ylim = c(0, 150))

abline(labor_model, 
       col = &quot;steelblue&quot;, 
       lwd = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre class="r"><code>detach(CPSSWEducation)</code></pre>
<p>We can plot the residuls to see if there is a Heteroskadasticity. Comparing those two plots, we can see the first one is clearly has Heteroskadasticity problem, the plotting residuals is fan shaped. But the second one is not very conclusive.</p>
<pre class="r"><code>plot(mod$residuals) #Evident by the picture</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<pre class="r"><code>plot(labor_model$residuals) #Inconclusive by the picture</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-36-2.png" width="672" /></p>
</div>
<div id="breausch-pagan-test" class="section level2">
<h2>Breausch-Pagan test</h2>
<p>For this test the NUll Hypothesis is: <strong>“The error variances are all equal”</strong></p>
<pre class="r"><code>library(lmtest)
bptest(labor_model) #NUll: The error variances are all equal</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  labor_model
## BP = 33.075, df = 1, p-value = 8.867e-09</code></pre>
<pre class="r"><code>bptest(mod)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  mod
## BP = 17.48, df = 1, p-value = 2.904e-05</code></pre>
<p>Both p-values are less than 0.05, we can consider it is statistically significant, the null hypothesis should be rejected. Hence, there is heteroskadasticity in both case.</p>
</div>
<div id="deal-with-heteroskadasticity" class="section level2">
<h2>Deal with heteroskadasticity</h2>
<p>Compute heteroskedasticity-robust standard errors. By using <code>vcovHC</code> function we can get the variance covariance matrix, then get the equare root of the variance covariance diagonal elements</p>
<pre class="r"><code>vcov &lt;- vcovHC(linear_model, type = &quot;HC1&quot;) #Var-cov matrix with robust s.e.
robust_se &lt;- sqrt(diag(vcov));robust_se #Robust s.e.</code></pre>
<pre><code>## (Intercept)         STR 
##  10.3643617   0.5194893</code></pre>
<pre class="r"><code>summary(linear_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***
## STR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06</code></pre>
<p>Carry out t-test between standard error and robust standard error. The Null hypothesis is: standard error and robust standard error are equal.</p>
<pre class="r"><code>coeftest(linear_model, vcov. = vcov) #t-test for coefficients using robust s.e.</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 698.93295   10.36436 67.4362 &lt; 2.2e-16 ***
## STR          -2.27981    0.51949 -4.3886 1.447e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Based on the test, we reject the Null hypothesis.</p>
</div>
<div id="another-example" class="section level2">
<h2>Another example</h2>
<p>For the model <span class="math inline">\(y_i=x_i+\mu_i\)</span>, <span class="math inline">\(\mu_i ~ N(0, 0.6x_i)\)</span></p>
<pre class="r"><code>set.seed(905)

# generate heteroskedastic data 
X &lt;- 1:500
Y &lt;- X+ rnorm(500,0,0.6*X) #Standard deviation depends on X</code></pre>
<pre class="r"><code># estimate a simple regression model
reg &lt;- lm(Y ~ X)

# plot the data
plot(x = X, y = Y, 
     pch = 19, 
     col = &quot;steelblue&quot;, 
     cex = 0.8)

# add the regression line to the plot
abline(reg, 
       col = &quot;red&quot;, 
       lwd = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Test if <span class="math inline">\(\beta_1=1\)</span></p>
<pre class="r"><code>linearHypothesis(reg, hypothesis.matrix = &quot;X = 1&quot;) #generic function to test a linear hypothesis</code></pre>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## X = 1
## 
## Model 1: restricted model
## Model 2: Y ~ X
## 
##   Res.Df      RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1    499 16386000                              
## 2    498 16257553  1    128447 3.9346 0.04785 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>ls(linearHypothesis(reg, hypothesis.matrix = &quot;X = 1&quot;)) #possible outputs</code></pre>
<pre><code>## [1] &quot;Df&quot;        &quot;F&quot;         &quot;Pr(&gt;F)&quot;    &quot;Res.Df&quot;    &quot;RSS&quot;       &quot;Sum of Sq&quot;</code></pre>
<p>We reject the null at 5% level Based on the p-value. This example tells us, because of the heteroskadasticity problem we might get the wrong conclusion.</p>
</div>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<p>Setting a confidence level of 95%, in about 5% of cases we will wrongly reject the null. However, the fraction of rejections will be greater if we do not use robust standard errors when errors are heteroskedastic!</p>
<p>Test this, via Monte Carlo Simulations!</p>
<p>Loop Routine:</p>
<ol style="list-style-type: decimal">
<li><p>Consider the simple linear regression model <span class="math inline">\(y=\beta_0+\beta_1x+\mu\)</span></p></li>
<li><p>Generate <span class="math inline">\(x\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(y\)</span> and violating the homoskedasticity assumption. Sample size=1000</p></li>
<li><p>Estimate the model</p></li>
<li><p>Perform t-test for B1 using non-robust and robust standard errors</p></li>
</ol>
<p>checking to false rejection.</p>
<p>Tip1:Use the function linearHypothesis()</p>
<p>Tip2: store the results in logic vectors</p>
<p>Compare the fraction of false rejections in each case. What are the implications of not using robust standard errors in this case?</p>
</div>
