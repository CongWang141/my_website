---
title: "Lesson 5"
author: "Cong Wang & Renan Serenini"
date: "2022-08-25"
draft: false
excerpt: 
weight: 1
links:
- icon: github
  icon_pack: fab
  name: code
  url: https://github.com/CongWang141/statistics-with-r.git
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### author
Cong Wang & Renan Serenini

## Solution to lesson 4

Setting a confidence level of 95%, in about 5% of cases we will wrongly reject the null. However, the fraction of rejections will be greater if we do not use robust standard errors when errors are heteroskedastic!

Test this, via Monte Carlo Simulations!

Loop Routine:

1. Consider the simple linear regression model $y=\beta_0+\beta_1x+\mu$

2. Generate $x$, $\mu$ and $y$ and violating the homoskedasticity assumption. Sample size=1000

3. Estimate the model

4. Perform t-test for B1 using non-robust and robust standard errors

checking to false rejection. 

Tip1:Use the function linearHypothesis()

Tip2: store the results in logic vectors

Compare the fraction of false rejections in each case. What are the implications of not using robust standard errors in this case?

```{r, message=FALSE}
library(car)
# initialize vectors t and t.rob
t <- c()
t.rob <- c()

# loop sampling and estimation
for (i in 1:1000) {
  # sample data
  X <- 1:1000
  u <- rnorm(1000, 0, 0.5*X)
  Y <- 5-3*X+u
  # estimate regression model
  reg <- lm(Y ~ X) 
  # homoskedasdicity-only significance test
  t[i] <- linearHypothesis(reg, "X = -3")$'Pr(>F)'[2] < 0.05 # WRONG REJECTION
  # robust significance test
  t.rob[i] <- linearHypothesis(reg, "X = -3", white.adjust = "hc1")$'Pr(>F)'[2] < 0.05
}

# compute the fraction of false rejections
cbind(t = mean(t), t.rob = mean(t.rob))
```

After using robust standard errors the probability of false rejection has declined. From 56% to 36%, not a big increase but better than nothing.

## Multiple regressors Linear model

```{r, message=FALSE}
# call packages
library(AER)
library(MASS)
library(mvtnorm)
```

We continue working on the dataset and the model we studied before. Let's consider our model with another variable, percenteage of English learners

```{r}
data(CASchools)

CASchools$STR <- CASchools$students/CASchools$teachers #create a new variable "student to teach ratio"       
CASchools$score <- (CASchools$read + CASchools$math)/2 #Score

mod <- lm(score ~ STR, data = CASchools) 
summary(mod)
```

Measure the correlations between chosen variables
```{r}
cor(CASchools$STR, CASchools$score) 
cor(CASchools$STR, CASchools$english) #Possible omitted variable bias. What if we estimate the model with both variables?
```

Estimate multiple regression model
```{r}
mult.mod <- lm(score ~ STR + english, data = CASchools)
summary(mult.mod)
```

## Measure of fit in multiple regression

```{r}
# define the components
n <- nrow(CASchools)                            # number of observations (rows)
k <- 2                                          # number of regressors

y_mean <- mean(CASchools$score)                 # mean of avg. test-scores

SSR <- sum(residuals(mult.mod)^2)               # sum of squared residuals
TSS <- sum((CASchools$score - y_mean )^2)       # total sum of squares
ESS <- sum((fitted(mult.mod) - y_mean)^2)       # explained sum of squares
```

## Compute the measures
```{r}
SER <- sqrt(1/(n-k-1) * SSR)                    # standard error of the regression
Rsq <- 1 - (SSR / TSS)                          # R^2
adj_Rsq <- 1 - (n-1)/(n-k-1) * SSR/TSS          # adj. R^2

# print the measures to the console
c("SER" = SER, "R2" = Rsq, "Adj.R2" = adj_Rsq)
```

## Ols assumption

Multicolinearity (2 or more regressors are strongly correlated). Perfect multicolinearity makes it impossible to estimate the model, for example:

```{r}
# define the fraction of English learners        
CASchools$FracEL <- CASchools$english / 100 #FracEL is a linear combination of 'english'

# estimate the model
mult.mod <- lm(score ~ STR + english + FracEL, data = CASchools) 

# obtain a summary of the model
summary(mult.mod) 
```

Another example:

```{r}
# if STR smaller 12, NS = 0, else NS = 1
CASchools$NS <- ifelse(CASchools$STR < 12, 0, 1) #IFELSE

# estimate the model
mult.mod <- lm(score ~ computer + english + NS, data = CASchools)

# obtain a model summary
summary(mult.mod) # see what will happen!

table(CASchools$NS)
```

## Bias-variance tradeoff 

New function: ```rmvnorm``` - Draw from multivariate normal density
```{r, results=FALSE}
?rmvnorm
rmvnorm(n=10, mean = c(1,3), sigma = cbind(c(1,5),c(5,2)))
#first column(X1)= N(1,1)
#second column(X2)= N(3,2)
#Cov(x1,x2)=5

var_cov=cbind(c(1,5),c(5,2));var_cov
```

Consider the model: $y=5+2.5x_1+sx_2+\mu$. Let's estimate it changing the value of Cov(X1,X2)

```{r}
# set number of observations
n <- 50

# initialize vectors of coefficients
coefs1 <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))
coefs2 <- coefs1

# set seed
set.seed(1)

# loop sampling and estimation
for (i in 1:10000) {
  
  # for cov(X_1,X_2) = 0.25
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs1[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
  # for cov(X_1,X_2) = 0.85
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 8.5), c(8.5, 10)))
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs2[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
}
```

```{r}
# obtain variance estimates
diag(var(coefs1)) #In this case, 
diag(var(coefs2))
cbind(c(10, 8.5), c(8.5, 10))
par(mfrow=c(1,2))
hist(coefs1[,1], xlim = c(1,4), ylim = c(0,2000))
hist(coefs2[,1], xlim = c(1,4), ylim = c(0,2000))
```

The plot of distribution of OLS estimators in multiple regression can tell us the probem brought by multicolinearity. When the correlation between $x_1$ and $x_2$ is small 0.25 on the left, the OLS estimator for $\beta_1$ is more arcuate. When the correlation is high 0.85 on the right, the OLS estimator is not biased but not efficient. The conclusion we can make from this example is that in a multiple regression if two or more independent variables are highly correlated the estimate for the coefficients are not arcuate. 

```{r}
# set sample size
n <- 50

# initialize vector of coefficients
coefs <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))

# set seed for reproducibility
set.seed(1)
```

```{r}
# loop sampling and estimation
for (i in 1:10000) {
  
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs[i,] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
}

# compute density estimate
kde <- kde2d(coefs[, 1], coefs[, 2])
```

```{r}
# plot the density estimate
persp(kde, 
      theta = 310, 
      phi = 30, 
      xlab = "beta_1", 
      ylab = "beta_2", 
      zlab = "Est. Density"
      ,ticktype="detailed")
```

```{r}
#Bivariate normal density 
library(mnormt)
x     <- seq(-5, 5, 0.25) 
y     <- seq(-5, 5, 0.25)
mu    <- c(0, 0)
sigma <- matrix(c(2, -1, -1, 2), nrow = 2)
f     <- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     <- outer(x, y, f) #Outer product. Arguments: X,Y, FUNCTION
persp(x, y, z, theta = -30, phi = 25, 
      shade = 0.75, col = "gold", expand = 0.5, r = 2, 
      ltheta = 25, ticktype = "detailed")
```

## Exercise

1. Load the dataset 'mtcars' (check the help for information about it)

2. Check the descriptive statistics of the variables and the correlation
3. Estimate a multiple linear model to identify the variables that impact the fuel consumption of a car
4. Test the assumption of heteroskedasticity

5. Interpret the results of the model

6. From the same model, include another variable and repeat the tests and interpretations

7. What was the effect of including this new variable? Would you keep it in the model?

8. Create a dummy variable and include it in your model, commenting the result.

9. Generate random data for a new car and predict the values of it's fuel consumption based on your model